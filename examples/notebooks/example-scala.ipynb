{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage with Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcoursierapi._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mres3_2\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mRepository\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  IvyRepository(file:/home/user/.ivy2/local/[organisation]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], dropInfoAttributes = false),\n",
       "  MavenRepository(https://repo1.maven.org/maven2),\n",
       "  MavenRepository(https://jitpack.io),\n",
       "  MavenRepository(file:///home/user/.m2/repository),\n",
       "  MavenRepository(file:///home/user/.m2/repository),\n",
       "  MavenRepository(file:///home/user/.m2/repository)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Add local maven repository to default coursier config\n",
    "// This will append the following local directory: \"file:///home/user/.m2/repository\"\n",
    "import coursierapi._\n",
    "\n",
    "interp.repositories.update(\n",
    "  interp.repositories() :+ \n",
    "  MavenRepository.of(\"file://\" + java.nio.file.Paths.get(sys.props(\"user.home\")).resolve(\".m2/repository\"))\n",
    ")\n",
    "interp.repositories()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                       \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                      \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Import/download from maven\n",
    "import $ivy.`org.apache.spark:spark-core_2.12:3.3.0`\n",
    "import $ivy.`org.apache.spark:spark-sql_2.12:3.3.0`\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/18 04:38:40 INFO SparkContext: Running Spark version 3.3.0\n",
      "23/11/18 04:38:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/11/18 04:38:40 INFO ResourceUtils: ==============================================================\n",
      "23/11/18 04:38:40 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/11/18 04:38:40 INFO ResourceUtils: ==============================================================\n",
      "23/11/18 04:38:40 INFO SparkContext: Submitted application: test\n",
      "23/11/18 04:38:40 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/11/18 04:38:40 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/11/18 04:38:40 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/11/18 04:38:40 INFO SecurityManager: Changing view acls to: user\n",
      "23/11/18 04:38:40 INFO SecurityManager: Changing modify acls to: user\n",
      "23/11/18 04:38:40 INFO SecurityManager: Changing view acls groups to: \n",
      "23/11/18 04:38:40 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/11/18 04:38:40 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(user); groups with view permissions: Set(); users  with modify permissions: Set(user); groups with modify permissions: Set()\n",
      "23/11/18 04:38:40 INFO Utils: Successfully started service 'sparkDriver' on port 44559.\n",
      "23/11/18 04:38:40 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/11/18 04:38:40 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/11/18 04:38:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/11/18 04:38:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/11/18 04:38:40 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/11/18 04:38:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f112fd7a-8795-4d29-9b01-d80a55b930b2\n",
      "23/11/18 04:38:40 INFO MemoryStore: MemoryStore started with capacity 880.5 MiB\n",
      "23/11/18 04:38:40 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/11/18 04:38:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/11/18 04:38:40 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "23/11/18 04:38:40 INFO Executor: Starting executor ID driver on host 530c32a6b399\n",
      "23/11/18 04:38:40 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/11/18 04:38:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41483.\n",
      "23/11/18 04:38:40 INFO NettyBlockTransferService: Server created on 530c32a6b399:41483\n",
      "23/11/18 04:38:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/11/18 04:38:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 530c32a6b399, 41483, None)\n",
      "23/11/18 04:38:40 INFO BlockManagerMasterEndpoint: Registering block manager 530c32a6b399:41483 with 880.5 MiB RAM, BlockManagerId(driver, 530c32a6b399, 41483, None)\n",
      "23/11/18 04:38:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 530c32a6b399, 41483, None)\n",
      "23/11/18 04:38:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 530c32a6b399, 41483, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@66126828"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.appName(\"test\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/18 04:39:04 INFO SparkContext: Starting job: show at cmd7.sc:1\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Got job 1 (show at cmd7.sc:1) with 2 output partitions\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Final stage: ResultStage 1 (show at cmd7.sc:1)\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Missing parents: List()\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at show at cmd7.sc:1), which has no missing parents\n",
      "23/11/18 04:39:04 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 880.5 MiB)\n",
      "23/11/18 04:39:04 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 880.5 MiB)\n",
      "23/11/18 04:39:04 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 530c32a6b399:41483 (size: 5.1 KiB, free: 880.5 MiB)\n",
      "23/11/18 04:39:04 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at cmd7.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "23/11/18 04:39:04 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "23/11/18 04:39:04 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (530c32a6b399, executor driver, partition 0, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
      "23/11/18 04:39:04 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (530c32a6b399, executor driver, partition 1, PROCESS_LOCAL, 4578 bytes) taskResourceAssignments Map()\n",
      "23/11/18 04:39:04 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
      "23/11/18 04:39:04 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
      "23/11/18 04:39:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1470 bytes result sent to driver\n",
      "23/11/18 04:39:04 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1470 bytes result sent to driver\n",
      "23/11/18 04:39:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 8 ms on 530c32a6b399 (executor driver) (1/2)\n",
      "23/11/18 04:39:04 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 8 ms on 530c32a6b399 (executor driver) (2/2)\n",
      "23/11/18 04:39:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/11/18 04:39:04 INFO DAGScheduler: ResultStage 1 (show at cmd7.sc:1) finished in 0.016 s\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/11/18 04:39:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "23/11/18 04:39:04 INFO DAGScheduler: Job 1 finished: show at cmd7.sc:1, took 0.019578 s\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(4).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
